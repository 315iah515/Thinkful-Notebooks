{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinkful Bootcamp Course\n",
    "\n",
    "Author: Ian Heaton\n",
    "\n",
    "Email: iheaton@gmail.com\n",
    "\n",
    "Mentor: Nemanja Radojkovic\n",
    "\n",
    "Date: 2017/04/28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sb.set_style('darkgrid')\n",
    "my_dpi = 96\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delicious Classification\n",
    "\n",
    "\n",
    "## Question:\n",
    "How well can Support Vector Machine perform Classification on recipe ratings? \n",
    "\n",
    "What we want to see is if we can use the ingredients, nutritional information and keyword list to predict the rating.\n",
    "\n",
    "\n",
    "### Data:\n",
    "Data set is drawn from the larger epicurious dataset, which has a collection of recipes, key terms and ingredients, and their ratings.\n",
    "\n",
    "The dataset can be [found on Kaggle](https://www.kaggle.com/hugodarwood/epirecipes).\n",
    "\n",
    "\n",
    "### Context:\n",
    "For someone writing a cookbook this could be really useful information that could help them choose which recipes to include because they're more likely to be enjoyed and therefore make the book more likely to be successful.\n",
    "\n",
    "### Content:\n",
    "Over 20k recipes listed by recipe rating, nutritional information and assigned category (sparse). Dataset contains 680 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observations : 20052\n"
     ]
    }
   ],
   "source": [
    "# Read CSV containing text data\n",
    "data_file = '/media/ianh/space/ThinkfulData/Epicurious/epi_r.csv'\n",
    "recipes = pd.read_csv(data_file)\n",
    "print(\"\\nObservations : %d\" % (recipes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calories    4117\n",
       "protein     4162\n",
       "fat         4183\n",
       "sodium      4119\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls of features\n",
    "null_count = recipes.isnull().sum()\n",
    "null_count[null_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fours features have a lot of missing data points. Our next question is, are these missing data points mostly within the same observations (same data frame index) or scatter throughout the data set?\n",
    "Our bodies are genetically drawn to food with higher portions of fat and protein so therefore retaining these features would perhaps help our model to classify recipe ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The missing data for the 4 features are roughly confined to the same 4188 indices\n"
     ]
    }
   ],
   "source": [
    "# adding indices of missing feature data into a Set to asses the dispersion of of missing data across \n",
    "# the entire data set\n",
    "big_mask = recipes.isnull()\n",
    "unique_rows = set(list(big_mask[big_mask.calories == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.protein == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.fat == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.sodium == True].index.values))\n",
    "print('\\nThe missing data for the 4 features are roughly confined to the same %d indices' % (len(unique_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we add the index values of the masks representing those cells where one of the features has a null value into the Set we are determining how distributed the missing data points of these observations are.  If the missing data is quite dispersed we will need to the drop the features from the data frame. \n",
    "\n",
    "With 4188 missing data points we can still use the four features if we are willing to settle for 15,864 observations to drive our Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are features with state names, those with column names starting with ‘#’ and others with dubious labels that may hold very little information, perhaps not worth the increased time in computation and the high bias in our model. Lets investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features after pruning and removing those features with zero variance: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>bake</th>\n",
       "      <th>bon appétit</th>\n",
       "      <th>dairy free</th>\n",
       "      <th>dessert</th>\n",
       "      <th>dinner</th>\n",
       "      <th>...</th>\n",
       "      <th>side</th>\n",
       "      <th>soy free</th>\n",
       "      <th>sugar conscious</th>\n",
       "      <th>summer</th>\n",
       "      <th>tomato</th>\n",
       "      <th>tree nut free</th>\n",
       "      <th>vegetable</th>\n",
       "      <th>vegetarian</th>\n",
       "      <th>wheat/gluten-free</th>\n",
       "      <th>winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.500</td>\n",
       "      <td>426.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.375</td>\n",
       "      <td>403.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.750</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.125</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.375</td>\n",
       "      <td>948.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  calories  protein   fat  sodium  bake  bon appétit  dairy free  \\\n",
       "0   2.500     426.0     30.0   7.0   559.0   0.0          0.0         0.0   \n",
       "1   4.375     403.0     18.0  23.0  1439.0   1.0          1.0         0.0   \n",
       "2   3.750     165.0      6.0   7.0   165.0   0.0          0.0         0.0   \n",
       "4   3.125     547.0     20.0  32.0   452.0   1.0          1.0         0.0   \n",
       "5   4.375     948.0     19.0  79.0  1042.0   0.0          1.0         0.0   \n",
       "\n",
       "   dessert  dinner   ...    side  soy free  sugar conscious  summer  tomato  \\\n",
       "0      0.0     0.0   ...     0.0       0.0              0.0     0.0     1.0   \n",
       "1      0.0     0.0   ...     0.0       0.0              0.0     0.0     0.0   \n",
       "2      0.0     0.0   ...     0.0       0.0              0.0     0.0     0.0   \n",
       "4      0.0     0.0   ...     1.0       0.0              0.0     0.0     0.0   \n",
       "5      0.0     0.0   ...     0.0       0.0              0.0     1.0     1.0   \n",
       "\n",
       "   tree nut free  vegetable  vegetarian  wheat/gluten-free  winter  \n",
       "0            0.0        1.0         0.0                0.0     0.0  \n",
       "1            0.0        0.0         0.0                0.0     1.0  \n",
       "2            0.0        1.0         0.0                0.0     0.0  \n",
       "4            0.0        1.0         1.0                0.0     0.0  \n",
       "5            0.0        0.0         0.0                0.0     0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove title feature and drop all NaN\n",
    "del recipes['title']\n",
    "recipes = recipes.dropna()\n",
    "\n",
    "# We have 680 features at this point\n",
    "# Now we will perform a rigorous approach of removing those features with zero variance.\n",
    "selector = VarianceThreshold(threshold=(.90 * (1 - .90)))\n",
    "selector.fit(recipes, recipes.rating)\n",
    "recipes = recipes.loc[:, selector.get_support()]\n",
    "\n",
    "\n",
    "print(\"Number of features after pruning and removing those features with zero variance: %d\" % (len(recipes.columns)))\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold used removed those features having zero values across 90% of all observations. Luckily the filtering process saved our nutritional observations. \n",
    "\n",
    "The variance threshold removed the state names, hashtag labels and those adding very little information.\n",
    "\n",
    "Next, we look at the rating feature and best decide how to create a new binary feature from which we will build our model to classify the recipes by.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    15864.000000\n",
      "mean         3.760952\n",
      "std          1.285518\n",
      "min          0.000000\n",
      "25%          3.750000\n",
      "50%          4.375000\n",
      "75%          4.375000\n",
      "max          5.000000\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAENCAYAAAA1/m/1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC1pJREFUeJzt3W9ol4Xex/Hvfpv5Z61YKbM/UqFEEBImZ3cmYpQ+MOuO\ngmwenZFCD0IPhiWxoE5hR1M6aXhXTD34IGW3D0ISgrB/Elm6Bz5IOyNuBTFGtvmnbC51c/eD+z4z\nD+VmzX33c6/XowmXl59dk7fXLrffSrq6uroCgDSF7AEAg50QAyQTYoBkQgyQTIgBkgkxQLKy3h7Y\n0dEZx46dvJRbikZl5QjX4v+5Fue4Fue4FueMGlXR4zG9viMuKyv9Q2MuJ67FOa7FOa7FOa7FxfFo\nAiCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQYIJkQAyQTYoBkQgyQTIgBkgkxQDIhBkgmxADJ\nhBggmRADJBNigGS9/uGhwOXvb3/7axw7dvQPn6e0tBCdnWf7YNGva2tri2HDhsXf//5fl+zP6E9C\nDHQ7duxoHDlyJEqGDM+eckFdZ9rj9OlT2TP6jBAD5ykZMjyuHPef2TMu6MQ//zt7Qp/yjBggmRAD\nJBNigGRCDJBMiAGSCTFAMiEGSCbEAMmEGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQYIJkQ\nAyQTYoBkQgyQTIgBkgkxQDIhBkgmxADJhBggmRADJBNigGRCDJBMiAGSCTFAMiEGSCbEAMmEGCCZ\nEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQYIJkQAyQTYugHW7Zsii1bNmXPoJ/19mMuxNAPGht3\nRWPjruwZ9LPefsyFGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQYIJkQAyQTYoBkQgyQTIgB\nkgkxQDIhBkgmxADJhBggmRADJBNigGRCDJBMiAGSCTFAMiEGSCbEAMmEGCCZEAMkE2KAZEIMkEyI\nAZIJMUAyIQZIJsQAyYQYIJkQAyQTYoBkQgyQTIgBkgkxQDIhBkjW6xA/+OCDl3LHoDJ//p9j/vw/\nZ8/oUTHsLIaN0BN3xADJLirE7jz+uF9ew4F8PYthZzFshN4oyx4Ag0FbW1ucPn0qnn32L9lTLujY\nsaPRVRSfKHfF2bNdRXE9e6MYrjjAZc0dMfSD8vLyKC8vj1Wr3sieckHPPvuXOPrjyewZvVAShUJJ\nUVzP3rioO+J//GPz7xrDOb+8hgP5ehbDzmLYCL3h0QRAsl4/mti2bVu0tJy4lFsGjWK5eyuGncWw\nEXrijhggmRADJBNigGRCDJBMiAGSCTFAMiEGSCbEAMmEGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZI\nJsQAyYQYIJkQAyQTYoBkQgyQTIgBkgkxQDIhBkgmxADJhBggmRADJBNigGRCDJBMiAGSCTFAMiEG\nSCbEAMmEGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAycqyB8Bg8Kc//Uf2BBL09uMuxNAPZs2a\nkz2BBL39uHs0AZBMiAGSCTFAMiEGSCbEAMmEGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQY\nIJkQAyQTYoBkQgyQTIgBkgkxQDIhBkgmxADJhBggmRADJBNigGRCDJBMiAGSCTFAMiEGSCbEAMmE\nGCCZEAMkE2KAZEIMkEyIAZIJMUAyIQZIJsQAyYQYIJkQAyQTYoBkQgyQrCx7ADCwdJ1pj5/+573s\nGT3oioiS7BF9RoiBbpWV1/TJeUpLC9HZebZPzvVr2tq6YtiwYZfs/P1NiIFudXV/7ZPzjBpVES0t\nJ/rkXIOBZ8QAyYQYIJkQAyQTYoBkQgyQTIgBkgkxQDIhBkgmxADJhBggmRADJBNigGRCDJBMiAGS\nCTFAMiEGSCbEAMmEGCCZEAMkE2KAZEIMkKykq6urK3sEwGDmjhggmRADJBNigGRCDJBMiAGSCTFA\nsl6FePny5VFTUxOzZ8+Or7766lJvGtC++eabmD59emzatCl7SrqVK1dGTU1NPProo7F9+/bsOSl+\n/vnnWLx4cdTW1sZjjz0Wn376afakdKdOnYpp06bF1q1bs6ek2b17d0yaNCnmzZsXtbW1sWzZsgse\nX9bTCRsbG+PgwYPR0NAQ+/fvj+effz4aGhr6bHAxaW9vj2XLlsWkSZOyp6TbtWtX7N+/PxoaGuL4\n8ePx8MMPx/Tp07Nn9buPP/44xo8fHwsWLIjm5uZ44okn4p577smelerNN9+MysrK7BnpqqurY82a\nNb06tscQf/HFFzFt2rSIiBg7dmz8+OOP0dbWFuXl5X9sZREaOnRorF+/Purr67OnpKuuro477rgj\nIiKuvvrqaG9vj66urigpKUle1r/uv//+7rebm5vjuuuuS1yT78CBA3HgwIGYOnVq9pR0F/O9cj0+\nmmhtbY1rrrmm+9eVlZXR2tr6+5YVuUKhEFdccUX2jAGhpKQkhg0bFhERW7ZsialTpw66CP9STU1N\nLF26NOrq6rKnpHr11Vfjueeey54xIOzfvz+eeuqpmDNnTuzcufOCx/Z4R/zvVR+Mdz38tg8//DDe\nfffd2LBhQ/aUVA0NDdHU1BTPPPNMvPfee9lzUmzdujUmTJgQN9xwQ0Rc3B3h5eamm26KhQsXxowZ\nM+LQoUMxb9682L59e5SV/XpyewxxVVXVeXfA33//fYwcObLvFlO0Pvvss6ivr48NGzbElVdemT0n\nxb59++Laa6+N0aNHx2233RadnZ1x9OjR8z6LHCx27NgR3377bXzyySfx3XffxdChQ2P06NGD8v9U\nqqqqYsaMGRERMWbMmBg5cmQcPny4+x+pf9djiCdPnhxr166NWbNmxddffx1VVVUxYsSIvl1N0fnp\np59i1apVsXHjxqioqMiek6axsTGam5ujrq4uWltbo729fVBGOCLi9ddf73577dq1ceONNw7KCEdE\nbNu2LVpaWmL+/PnR0tISR44ciaqqqt88vscQT5gwIW6//faoqamJ0tLSeOGFF/p0cDHZt29frFix\nIpqbm6OsrCw++OCDWLt2bVx11VXZ0/rd+++/H8ePH4/Fixd3P65auXJljB49Ontav5o9e3bU1dXF\nnDlz4tSpU/Hiiy9mT2IAuPfee2PJkiXx0UcfRUdHR7z00ku/+VgiwstgAqTznXUAyYQYIJkQAyQT\nYoBkQgyQTIgBkgkxRelf30bc1NTU40sMwkDn64gpOocPH46nn346Nm/enD0F+oQQM6Ds3r073nrr\nrRg6dGhMnDgxvvzyy+js7IwTJ07EvHnz4qGHHora2tpoamqK++67Lx555JFYvXp1bN68OWpra+Pu\nu++OPXv2xMGDB2PRokXxwAMPxKFDh2Lp0qVRKBRi/PjxsWPHjqivr48xY8Zkv7sQER5NMADt3bs3\nVq1aFVOmTIm5c+fGxo0b4+23347ly5dHRMSiRYvi1ltvjRUrVkREnPdqgO3t7VFfXx/Lli2L9evX\nR0TEG2+8ETNnzoxNmzbF5MmT4+DBg/3/TsEF9PhaE9DfbrnllqioqIhRo0bFunXrYt26dVFaWho/\n/PBDj7+3uro6IiKuv/767uObmpriySefjIiIKVOmxPDhwy/dePgdhJgBZ8iQIRERsXr16rj55pvj\ntddei5MnT8bEiRN7/L2lpaXdb//rqdvZs2fPu2suFHwiyMDibyQDVmtra4wbNy4i/u9lBQuFQpw5\ncyYKhUJ0dHT0+jxjx46NPXv2RETE559/HidPnrwke+H3EmIGrLlz58aaNWtiwYIFUVFREXfddVcs\nWbIkxo0bFy0tLbFgwYLzjv+tnxyzcOHCeOedd+Lxxx+P3bt3R1VV1Xl3zpDNV01w2du7d2+cPn06\n7rzzzmhtbY2ZM2fGzp07xZgBwzNiLnsjRoyIV155JSIiOjo64uWXXxZhBhR3xADJPCMGSCbEAMmE\nGCCZEAMkE2KAZEIMkOx/AWwkfGIuBei3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb78d57fd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(recipes.rating.describe())\n",
    "__ = sb.boxplot(recipes.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that a good bifurcation point is 4.5 just above the 4th quartile.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of observations that are delicious: 2106\n"
     ]
    }
   ],
   "source": [
    "recipes['collapsedrating'] = np.where(recipes.rating < 4.5, 0, 1)\n",
    "print('The amount of observations that are delicious: %d' % ((recipes.collapsedrating == 1).sum()))\n",
    "\n",
    "# Lets remove the rating feature before we build the classifier\n",
    "del recipes['rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the new feature there are 2106 observations that are classified as delicious and 13,758 observations that are NOT delicious.  This gives an unbalanced dataset which is fine, good food is hard to come by. \n",
    "\n",
    "Lastly we scale the four nutritional features so that those features with greater ranges do not over influence the support vectors (their distance fro their peers across the decision boundary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale our continuous data\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "recipes['calories_scaled'] = min_max_scaler.fit_transform(recipes.calories)\n",
    "recipes['protein_scaled'] = min_max_scaler.fit_transform(recipes.protein)\n",
    "recipes['fat_scaled'] = min_max_scaler.fit_transform(recipes.fat)\n",
    "recipes['sodium_scaled'] = min_max_scaler.fit_transform(recipes.sodium)\n",
    "\n",
    "# Remove the original features we don’t want them to be added our model, making our scaling useless \n",
    "recipes = recipes.drop(['calories', 'protein', 'fat', 'sodium'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function creates decision boundary plot and draws test samples if present \n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                         np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot all samples\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]                               \n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, c=cmap(idx),\n",
    "                    marker=markers[idx], label=cl)\n",
    "        \n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]   \n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='', \n",
    "                alpha=1.0, linewidth=1, marker='o', \n",
    "                s=55, label='test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations is : 15864 and the number of features are: 31\n",
      "\n",
      "Accuracy on training set: 0.865\n",
      "\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X = recipes.drop(['collapsedrating'], 1)\n",
    "print('The number of observations is : %d and the number of features are: %d' % (X.shape[0], len(X.columns)))\n",
    "y = recipes['collapsedrating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "svm = SVC(C=1.0, random_state=0)\n",
    "svm.fit(X_train, y_train)\n",
    "#y_ = svm.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy on training set: {:.3f}\".format(svm.score(X_train, y_train)))\n",
    "print(\"\\nAccuracy on test set: {:.3f}\".format(svm.score(X_test, y_test)))\n",
    "\n",
    "X_combined = np.vstack((X_train, X_test))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "#plot_decision_regions(X_combined, y_combined, classifier=svm)\n",
    "\n",
    "#plt.xlabel('Delicious')\n",
    "#plt.ylabel('Not Delicious')\n",
    "#plt.legend(loc='upper left')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
