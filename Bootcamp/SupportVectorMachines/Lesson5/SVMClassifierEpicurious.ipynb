{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinkful Bootcamp Course\n",
    "\n",
    "Author: Ian Heaton\n",
    "\n",
    "Email: iheaton@gmail.com\n",
    "\n",
    "Mentor: Nemanja Radojkovic\n",
    "\n",
    "Date: 2017/04/28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import us\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sb.set_style('darkgrid')\n",
    "my_dpi = 96\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delicious Classification\n",
    "\n",
    "\n",
    "## Question:\n",
    "How well can Support Vector Machine perform Classification on recipe ratings? \n",
    "\n",
    "What we want to see is if we can use the ingredients, nutritional information and keyword list to predict the rating.\n",
    "\n",
    "\n",
    "### Data:\n",
    "Data set is drawn from the larger epicurious dataset, which has a collection of recipes, key terms and ingredients, and their ratings.\n",
    "\n",
    "The dataset can be [found on Kaggle](https://www.kaggle.com/hugodarwood/epirecipes).\n",
    "\n",
    "\n",
    "### Context:\n",
    "For someone writing a cookbook this could be really useful information that could help them choose which recipes to include because they're more likely to be enjoyed and therefore make the book more likely to be successful.\n",
    "\n",
    "### Content:\n",
    "Over 20k recipes listed by recipe rating, nutritional information and assigned category (sparse). Dataset contains 680 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observations : 20052\n"
     ]
    }
   ],
   "source": [
    "# Read CSV containing text data\n",
    "data_file = '/media/ianh/space/ThinkfulData/Epicurious/epi_r.csv'\n",
    "recipes = pd.read_csv(data_file)\n",
    "print(\"\\nObservations : %d\" % (recipes.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calories    4117\n",
       "protein     4162\n",
       "fat         4183\n",
       "sodium      4119\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count nulls of features\n",
    "null_count = recipes.isnull().sum()\n",
    "null_count[null_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fours features have a lot of missing data points. Our next question is, are these missing data points mostly within the same observations (same data frame index) or scatter throughout the data set?\n",
    "Our bodies are genetically drawn to food with higher portions of fat and protein so therefore retaining these features would perhaps help our model to classify recipe ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The missing data for the 4 features are roughly confined to the same 4188 indices\n"
     ]
    }
   ],
   "source": [
    "# adding indices of missing feature data into a Set to asses the dispersion of of missing data across \n",
    "# the entire data set\n",
    "big_mask = recipes.isnull()\n",
    "unique_rows = set(list(big_mask[big_mask.calories == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.protein == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.fat == True].index.values))\n",
    "unique_rows.update(list(big_mask[big_mask.sodium == True].index.values))\n",
    "print('\\nThe missing data for the 4 features are roughly confined to the same %d indices' % (len(unique_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we add the index values of the masks representing those cells where one of the features has a null value into the Set we are determining how distributed the missing data points of these observations are.  If the missing data is quite dispersed we will need to the drop the features from the data frame. \n",
    "\n",
    "With 4188 missing data points we can still use the four features if we are willing to settle for 15,864 observations to drive our Support Vector Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are features with state names, those with column names starting with ‘#’ and others with dubious labels that may hold very little information, perhaps not worth the increased time in computation and the high bias in our model. Lets investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features after pruning: 570\n"
     ]
    }
   ],
   "source": [
    "# List will be used to exclude those features that are of type str or one of the four nutritional \n",
    "# type that we are trying to keep\n",
    "excluded = ['title', 'calories', 'protein', 'fat', 'sodium']\n",
    "\n",
    "# function iterates for all column names and records those that have less than 5 non zero values\n",
    "# This should catch those column’s names that start with ‘#’\n",
    "def create_columns_filter(column_names, threshold=5):\n",
    "    result = []\n",
    "    for label in column_names:\n",
    "        if label not in excluded:\n",
    "            number = np.sum(recipes[label] > 0)\n",
    "            if number < threshold:\n",
    "                result.append(label)\n",
    "    return result\n",
    "\n",
    "# Find all columns which meet our above criteria\n",
    "colunms_tobe_dropped = create_columns_filter(recipes.columns)\n",
    "\n",
    "# Lets add the 'title' column to the list of features to be removed\n",
    "colunms_tobe_dropped.insert(0, 'title')\n",
    "\n",
    "# function that removes features from passed dataframe whose names appear\n",
    "# within the cols parameter\n",
    "def prune(cols, dataframe):\n",
    "    for name in cols:\n",
    "        dataframe.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "# Prune baby prune\n",
    "prune(colunms_tobe_dropped, recipes)\n",
    "print(\"Number of features after pruning: %d\" % (len(recipes.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
